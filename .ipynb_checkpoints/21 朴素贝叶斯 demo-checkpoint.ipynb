{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建 TfidfVectorizer 类：\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = [\n",
    "    'this is the bayes document',\n",
    "    'this is the second second document',\n",
    "    'and the third one',\n",
    "    'is this the document'\n",
    "]\n",
    "tfidf_matrix = tfidf_vec.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不重复的词: ['and', 'bayes', 'document', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "#输出文档中所有不重复的词：\n",
    "print('不重复的词:', tfidf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个单词的ID: {'this': 8, 'is': 3, 'the': 6, 'bayes': 1, 'document': 2, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n"
     ]
    }
   ],
   "source": [
    "#输出每个单词对应的 id 值：\n",
    "print('每个单词的ID:', tfidf_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个单词的tfidf值: [[0.         0.63314609 0.40412895 0.40412895 0.         0.\n",
      "  0.33040189 0.         0.40412895]\n",
      " [0.         0.         0.27230147 0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.         0.52210862 0.52210862 0.         0.\n",
      "  0.42685801 0.         0.52210862]]\n"
     ]
    }
   ],
   "source": [
    "#输出每个单词在每个文档中的 TF-IDF 值，向量里的顺序是按照词语的 id 顺序来的：\n",
    "print('每个单词的tfidf值:', tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何对文档进行分类\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7c1d37b28111>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#分词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#标注单词的词性\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "#1对文档进行分词 英文 和中文\n",
    "\n",
    "import nltk\n",
    "word_list = nltk.word_tokenize(text) #分词\n",
    "nltk.pos_tag(word_list) #标注单词的词性\n",
    "\n",
    "\n",
    "import jieba\n",
    "word_list = jieba.cut (text) #中文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2加载停用词\n",
    "\n",
    "stop_words = [line.strip().decode('utf-8') for line in io.open('stop_words.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3计算单词权重\n",
    "\n",
    "tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)\n",
    "features = tf.fit_transform(train_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4生成朴素贝叶斯分类器\n",
    "\n",
    "# 多项式贝叶斯分类器\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5使用生成的分类器做预测\n",
    "\n",
    "test_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=train_vocabulary)\n",
    "test_features=test_tf.fit_transform(test_contents)\n",
    "\n",
    "\n",
    "predicted_labels=clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6计算准确率\n",
    "\n",
    "from sklearn import metrics\n",
    "print metrics.accuracy_score(test_labels, predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
